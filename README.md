# Text-Image-SSL
 
A collection of recent papers on text-image Self-supervised Learning.

## Baseline Methods
- [Masked Autoencoders Are Scalable Vision Learners (MAE)](https://arxiv.org/pdf/2111.06377.pdf)
- [Learning Transferable Visual Models From Natural Language Supervision (CLIP)](https://arxiv.org/pdf/2103.00020.pdf)
- [Zero-Shot Text-to-Image Generation (DALLE)](https://arxiv.org/abs/2102.12092.pdf)
- [Hierarchical Text-Conditional Image Generation with CLIP Latents (DALLE2)](https://arxiv.org/pdf/2204.06125.pdf)

## Related Works
- [Training Vision-Language Transformers from Captions Alone](https://arxiv.org/pdf/2205.09256.pdf)
- [An Empirical Study of Training End-to-End Vision-and-Language Transformers](https://arxiv.org/pdf/2111.02387.pdf)
- [MulT: An End-to-End Multitask Learning Transformer](https://arxiv.org/pdf/2205.08303.pdf)
- [MultiMAE: Multi-modal Multi-task Masked Autoencoders](https://arxiv.org/pdf/2204.01678.pdf)
- [How Much Can CLIP Benefit Vision-and-Language Tasks?](https://arxiv.org/pdf/2107.06383.pdf)
- [ViLT: Vision-and-Language Transformer Without Convolution or Region Supervision](https://arxiv.org/pdf/2102.03334.pdf)
- [mPLUG: Effective and Efficient Vision-Language Learning by Cross-modal Skip-connections](https://arxiv.org/pdf/2205.12005.pdf)


## Dataset:
- [Flicker30k](https://shannon.cs.illinois.edu/DenotationGraph/)
- [MSCOCO](https://cocodataset.org/#home)
- [LAION-300M](https://arxiv.org/pdf/2111.02114.pdf)
